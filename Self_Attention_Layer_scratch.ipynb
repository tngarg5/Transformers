{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YkNd32kBc3LX",
        "outputId": "f6274947-a891-4372-9e72-564abe87687a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "d_model:  3\n",
            "scores:  [[0.23597567 0.50181978 0.4517456  0.49794841 0.25514919]\n",
            " [0.57074441 0.43283436 0.75930215 0.39959128 0.57724248]\n",
            " [0.17780042 0.53484149 0.37844119 0.53948227 0.21710484]\n",
            " [0.30004891 0.16501067 0.35482251 0.14731527 0.31059413]\n",
            " [0.17739261 0.31114326 0.17613404 0.31917965 0.26748215]]\n",
            "attention_weights:  [[0.1705263  0.22245696 0.21159191 0.22159741 0.17382743]\n",
            " [0.20292638 0.1767848  0.24503498 0.17100454 0.2042493 ]\n",
            " [0.1632091  0.23324124 0.19947182 0.23432617 0.16975167]\n",
            " [0.20837921 0.18205727 0.22011126 0.17886403 0.21058824]\n",
            " [0.18558201 0.21214023 0.18534858 0.21385194 0.20307724]]\n",
            "Query Matrix (Q):\n",
            " [[0.34389445 0.84954018 0.22716043]\n",
            " [0.15460511 0.66601988 0.96000454]\n",
            " [0.48509182 0.81343594 0.06162514]\n",
            " [0.08220974 0.17436809 0.52095687]\n",
            " [0.62363214 0.02672537 0.07829541]]\n",
            "Key Matrix (K):\n",
            " [[0.37479224 0.08617332 0.90960026]\n",
            " [0.81081018 0.63966366 0.20656881]\n",
            " [0.34752642 0.52670637 0.94856179]\n",
            " [0.84111984 0.63620184 0.14411171]\n",
            " [0.62562712 0.01891634 0.92758883]]\n",
            "Value Matrix (V):\n",
            " [[0.77908863 0.45255078 0.75476574]\n",
            " [0.51467933 0.50670193 0.6431074 ]\n",
            " [0.32842568 0.02073447 0.66202566]\n",
            " [0.08008641 0.4600139  0.68185068]\n",
            " [0.13564337 0.98427859 0.75665663]]\n",
            "Attention Output:\n",
            " [[0.35816679 0.46731093 0.69447442]\n",
            " [0.3709611  0.46619504 0.70021909]\n",
            " [0.3545025  0.47105636 0.69345845]\n",
            " [0.37122665 0.48067226 0.70138071]\n",
            " [0.35931497 0.49357969 0.69868024]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "def self_attention(Q, K, V):\n",
        "    \"\"\"\n",
        "    Self-Attention mechanism for a single attention head.\n",
        "\n",
        "    Parameters:\n",
        "    - Q (ndarray): Query matrix (sequence_length, d_model).\n",
        "    - K (ndarray): Key matrix (sequence_length, d_model).\n",
        "    - V (ndarray): Value matrix (sequence_length, d_model).\n",
        "\n",
        "    Returns:\n",
        "    - Attention output (ndarray): Weighted sum of values based on attention scores.\n",
        "    \"\"\"\n",
        "    d_model = Q.shape[-1]\n",
        "    print('d_model: ',d_model)\n",
        "\n",
        "    # Calculate attention scores (scaled dot-product attention)\n",
        "    scores = np.matmul(Q, K.T) / np.sqrt(d_model)\n",
        "    print('scores: ',scores)\n",
        "    # Apply softmax to obtain attention weights\n",
        "    attention_weights = np.exp(scores - np.max(scores, axis=-1, keepdims=True))\n",
        "    attention_weights /= np.sum(attention_weights, axis=-1, keepdims=True)\n",
        "    print('attention_weights: ',attention_weights)\n",
        "    # Calculate attention output as a weighted sum of values\n",
        "    attention_output = np.matmul(attention_weights, V)\n",
        "\n",
        "    return attention_output\n",
        "\n",
        "# Example usage\n",
        "sequence_length = 5\n",
        "d_model = 3\n",
        "\n",
        "# Example query, key, and value matrices\n",
        "Q = np.random.rand(sequence_length, d_model)\n",
        "K = np.random.rand(sequence_length, d_model)\n",
        "V = np.random.rand(sequence_length, d_model)\n",
        "\n",
        "# Apply self-attention\n",
        "attention_output = self_attention(Q, K, V)\n",
        "\n",
        "print(\"Query Matrix (Q):\\n\", Q)\n",
        "print(\"Key Matrix (K):\\n\", K)\n",
        "print(\"Value Matrix (V):\\n\", V)\n",
        "print(\"Attention Output:\\n\", attention_output)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WDiAscgvc8vP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}